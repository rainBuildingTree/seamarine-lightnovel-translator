import json
import csv
import time
from ebooklib import epub
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import ast

# ì¶”ê°€: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
MAX_RETRIES = 3

# ì¶”ê°€: ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ "retry after X" í˜•íƒœë¡œ ì‹œê°„(ì´ˆ)ì„ íŒŒì‹±í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def parse_retry_delay_from_error(error) -> int:
    """
    ì£¼ì–´ì§„ ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì¬ì‹œë„ ì§€ì—°ì‹œê°„(ì´ˆ)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì—ëŸ¬ ë©”ì‹œì§€ì— "retry after <ìˆ«ì>" í˜•íƒœê°€ ìˆë‹¤ë©´ í•´ë‹¹ ìˆ«ìë¥¼ ë°˜í™˜í•˜ê³ ,
    ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ 10ì´ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        match = re.search(r"retry after (\d+)", str(error), re.IGNORECASE)
        if match:
            return int(match.group(1))
    except Exception:
        pass
    return 10
'''
def get_retry_delay_from_exception(e: Exception, extra_seconds: int = 2) -> int:
    """
    429 RESOURCE_EXHAUSTED ì—ëŸ¬ ë©”ì‹œì§€(429 QuotaExceeded)ì—ì„œ retryDelay(ì˜ˆ: "50s")ë¥¼ ì¶”ì¶œí•´
    ì´ˆ ë‹¨ìœ„ intë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ë§Œì•½ í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    :param e: ì˜ˆì™¸ ê°ì²´
    :param extra_seconds: retryDelayì— ì¶”ê°€ë¡œ ë” ê¸°ë‹¤ë¦¬ê³  ì‹¶ì€ ì´ˆ (ê¸°ë³¸ê°’ 1ì´ˆ)
    :return: ëŒ€ê¸°í•´ì•¼ í•  ì´ ì´ˆ (retryDelay + extra_seconds). í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ 0.
    """
    error_str = str(e)
    start_idx = error_str.find('{')
    retry_seconds = 0

    # JSON ë¬¸ìì—´ ë¶€ë¶„ ì¶”ì¶œ
    if start_idx != -1:
        json_str = error_str[start_idx:]

        try:
            err_data = json.loads(json_str)
            # details ì°¾ì•„ì„œ type.googleapis.com/google.rpc.RetryInfo ë¶€ë¶„ í™•ì¸
            details = err_data["error"].get("details", [])
            for detail in details:
                if detail.get("@type") == "type.googleapis.com/google.rpc.RetryInfo":
                    retry_str = detail.get("retryDelay", "")  # ì˜ˆ: "50s"
                    match = re.match(r"(\d+)s", retry_str)
                    if match:
                        retry_seconds = int(match.group(1))
                        break
        except json.JSONDecodeError:
            print(f"[ERROR] JSON decode error: {error_str}")
            return 10

    return retry_seconds + extra_seconds if retry_seconds > 0 else 0
'''
def get_retry_delay_from_exception(error_str: str, extra_seconds: int = 2) -> int:
    """
    error_str ì•ˆì— í¬í•¨ëœ Python-style dict ë¬¸ìì—´ì—ì„œ 'retryDelay' ê°’ì„ ì¶”ì¶œí•´ ì´ˆ(int)ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    :param error_str: ì—ëŸ¬ ë¬¸ìì—´
    :param extra_seconds: ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ (ê¸°ë³¸ê°’ 1ì´ˆ)
    :return: retryDelay + extra_seconds (ì—†ìœ¼ë©´ 0)
    """
    start_idx = error_str.find('{')
    if start_idx == -1:
        return 0  # JSON ë¶€ë¶„ ì—†ìŒ
    
    dict_str = error_str[start_idx:]
    retry_seconds = 0

    try:
        # JSONì´ ì•„ë‹ˆë¼ Python dictì²˜ëŸ¼ ìƒê¸´ ë¬¸ìì—´ì´ë¯€ë¡œ literal_eval ì‚¬ìš©
        err_data = ast.literal_eval(dict_str)
        details = err_data.get("error", {}).get("details", [])
        for detail in details:
            if detail.get("@type") == "type.googleapis.com/google.rpc.RetryInfo":
                retry_str = detail.get("retryDelay", "")  # ì˜ˆ: "39s"
                match = re.match(r"(\d+)s", retry_str)
                if match:
                    retry_seconds = int(match.group(1))
                    break
    except Exception as e:
        print(f"[WARN] retryDelay íŒŒì‹± ì‹¤íŒ¨: {e}")
        return 10

    return retry_seconds + extra_seconds if retry_seconds > 0 else 0


class ProperNounExtractor:
    def __init__(self, epub_path: str, client, llm_model: str, language: str = 'Japanese'):
        self.epub_path = epub_path
        self.client = client
        self.llm_model = llm_model
        self.language = language
        self.prompt = f'''You are a text processor that extracts and translates proper nouns from a {self.language} light novel text into Korean.

ğŸ¯ Task:
- Identify all proper nouns in the input text.
- Translate each proper noun into Korean.
- Return the result strictly as a JSON object.
- Use the original {self.language} proper noun as the key, and the Korean translation as the value.

âš ï¸ Output Requirements:
- Respond **only** with the JSON object.
- Do **not** include any commentary, explanation, or formatting outside the JSON.
- Follow this format exactly:

{{
  "ç”°ä¸­": "ë‹¤ë‚˜ì¹´",
  "æ±äº¬": "ë„ì¿„"
}}

ğŸ“„ The input text is below:
'''
        self.proper_nouns = {}

    def extract_text(self) -> str:
        book = epub.read_epub(self.epub_path)
        full_text = ""
        for item in book.get_items():
            if isinstance(item, epub.EpubHtml):
                try:
                    content = item.get_content().decode('utf-8')
                    soup = BeautifulSoup(content, 'lxml-xml')
                    full_text += soup.get_text() + '\n'
                except Exception as e:
                    print(f"[ERROR] Content decode failed: {e}")
        return full_text

    def split_text(self, text: str, max_length: int = 2000) -> list[str]:
        return [text[i:i+max_length] for i in range(0, len(text), max_length)]

    def clean_response(self, response_text: str) -> str:
        text = response_text.strip()
        if text.startswith("```json"):
            text = text[7:].strip()
        elif text.startswith("```"):
            text = text[3:].strip()
        if text.endswith("```"):
            text = text[:-3].strip()
        return text

    def process_chunk(self, chunk, delay):
        """
        chunk ë¬¸ìì—´ì„ Gemini APIì— ë³´ë‚´ proper nounì„ ì¶”ì¶œí•˜ê³ 
        429 (Resource exhausted) ì—ëŸ¬ ë°œìƒ ì‹œ ë™ì ìœ¼ë¡œ ì¬ì‹œë„ ì§€ì—°ì‹œê°„ì„ ì ìš©í•˜ì—¬
        ìµœëŒ€ MAX_RETRIESë§Œí¼ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
        """
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                response = self.client.models.generate_content(
                    model=self.llm_model,
                    contents=self.prompt + chunk
                )
                cleaned = self.clean_response(response.text.strip())
                new_dict = json.loads(cleaned)

                # ì •ìƒ ì‘ë‹µì„ ë°›ì€ ê²½ìš°, ìš”ì²­ í›„ ì§€ì—° (ê¸°ì¡´ delay) ì ìš©
                time.sleep(delay)
                return new_dict

            except Exception as e:
                print(f"[ERROR] Attempt {attempt} - Failed to parse chunk: {e}")

                # 429 í˜¹ì€ Resource exhausted ì—ëŸ¬ë¼ë©´ parse_retry_delay_from_error ì‚¬ìš©
                if "429" in str(e) or "Resource exhausted" in str(e):
                    attempt -= 1
                    dynamic_delay = get_retry_delay_from_exception(str(e))
                    print(f"  => 429/Resource exhausted ì—ëŸ¬ ê°ì§€. {dynamic_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                    time.sleep(dynamic_delay)
                else:
                    # ê·¸ ì™¸ ì—ëŸ¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì ì§„ì  ì¦ê°€
                    time.sleep(5 * attempt)

        print("[ERROR] Final failure after MAX_RETRIES attempts. Returning empty dict.")
        return {}

    def run_extraction(self, delay, max_concurrent_requests=1, progress_callback=None):
        full_text = self.extract_text()
        chunks = self.split_text(full_text)
        total_chunks = len(chunks)
        completed = 0

        with ThreadPoolExecutor(max_workers=max_concurrent_requests) as executor:
            # ê° ì²­í¬ì— ëŒ€í•´ process_chunk í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ì‘ì—… ì œì¶œ
            futures = {
                executor.submit(self.process_chunk, chunk, delay): chunk
                for chunk in chunks
            }
            for future in as_completed(futures):
                result = future.result()
                # ê²°ê³¼ë¥¼ self.proper_nounsì— ì—…ë°ì´íŠ¸
                self.proper_nouns.update(result)
                completed += 1
                if progress_callback:
                    progress_callback(int(completed / total_chunks * 100))

    def save_to_csv(self, filename="proper_nouns.csv"):
        with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(["ì›ì–´", "í•œêµ­ì–´"])
            for origin, kr in self.proper_nouns.items():
                writer.writerow([origin, kr])
