import asyncio
import copy
import logging
import PIL.Image
from bs4 import BeautifulSoup
from chunker import HtmlChunker
import re 
import time
import PIL
import io
import ast
from google.genai import types
import html

# Constants
MAX_RETRIES = 3
CASE_SENSITIVE_ATTRS = {
    # SVG ìš”ì†Œ ê´€ë ¨ ì†ì„±
    "viewbox": "viewBox",
    "preserveaspectratio": "preserveAspectRatio",
    "gradienttransform": "gradientTransform",
    "gradientunits": "gradientUnits",
    "patterntransform": "patternTransform",
    "markerstart": "markerStart",
    "markermid": "markerMid",
    "markerend": "markerEnd",
    "clippathunits": "clipPathUnits",
    "refx": "refX",
    "refy": "refY",
    "spreadmethod": "spreadMethod",
    "textlength": "textLength",
    "lengthadjust": "lengthAdjust",
    
    # SVG ì• ë‹ˆë©”ì´ì…˜ ê´€ë ¨ ì†ì„±
    "calcmode": "calcMode",
    "keytimes": "keyTimes",
    "keysplines": "keySplines",
    "repeatcount": "repeatCount",
    "repeatdur": "repeatDur",
    "attributename": "attributeName",
    
    # MathML ê´€ë ¨ ì†ì„± (ì‚¬ìš©ë˜ëŠ” ê²½ìš°)
    "definitionurl": "definitionURL",
    
    # EPUB/OPF ë©”íƒ€ë°ì´í„°ë‚˜ EPUB ì „ìš© ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì†ì„±
    # (EPUB 3ì—ì„œëŠ” epub:type ë“±ì€ ì¼ë°˜ì ìœ¼ë¡œ ì†Œë¬¸ìì§€ë§Œ, í˜¹ì‹œ ëª°ë¼ ì¶”ê°€)
    "epub:type": "epub:type",
    "unique-identifier": "unique-identifier",
}
def restore_case_sensitive_attribs(html_content):
    for lower_attr, correct_attr in CASE_SENSITIVE_ATTRS.items():
        # ì •ê·œì‹ìœ¼ë¡œ ì†Œë¬¸ì í˜•íƒœì˜ ì†ì„±ëª…ì„ ì°¾ì•„ ì˜¬ë°”ë¥¸ ì¼€ì´ìŠ¤ë¡œ ë³€ê²½
        pattern = re.compile(fr'(?P<before><\w+[^>]*\s){lower_attr}(?P<after>\s*=)', re.IGNORECASE)
        html_content = pattern.sub(lambda m: m.group("before") + correct_attr + m.group("after"), html_content)
    return html_content

def revert_foreign_paragraphs(original_html, translated_html):
    """
    ìµœì¢… ë²ˆì—­ ê²°ê³¼(HTML)ì—ì„œ ê° <p> íƒœê·¸ë¥¼ ê²€ì‚¬í•˜ì—¬,
    ë§Œì•½ í•´ë‹¹ ë¬¸ë‹¨ ë‚´ì— ì¼ë³¸ì–´, í‚¤ë¦´, íƒœêµ­, ì•„ë, íˆë¸Œë¦¬, ë°ë°”ë‚˜ê°€ë¦¬, ê·¸ë¦¬ìŠ¤ ë“±ì˜ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
    ì›ë¬¸ HTMLì˜ ëŒ€ì‘ <p> íƒœê·¸ ë‚´ìš©ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ ,
    ìµœì¢…ì ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ <html>ë‚˜ <body> íƒœê·¸ ì—†ì´ ìˆœìˆ˜í•œ HTML fragmentë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    pattern = re.compile(
        r'[\u3040-\u30FF\u31F0-\u31FF\u4E00-\u9FFF]|'  # ì¼ë³¸ì–´/í•œì
        r'[\u0400-\u04FF\u0500-\u052F]|'                # í‚¤ë¦´ ë¬¸ì
        r'[\u0E00-\u0E7F]|'                            # íƒœêµ­ ë¬¸ì
        r'[\u0600-\u06FF\u0750-\u077F]|'                # ì•„ë ë¬¸ì
        r'[\u0590-\u05FF]|'                            # íˆë¸Œë¦¬ ë¬¸ì
        r'[\u0900-\u097F]|'                            # ë°ë°”ë‚˜ê°€ë¦¬ ë¬¸ì
        r'[\u0370-\u03FF]'                             # ê·¸ë¦¬ìŠ¤ ë¬¸ì
    )

    # "html.parser"ë¥¼ ì‚¬ìš©í•˜ì—¬ fragmentë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    original_soup = BeautifulSoup(original_html, "html.parser")
    translated_soup = BeautifulSoup(translated_html, "html.parser")

    original_container = original_soup.body if original_soup.body is not None else original_soup
    translated_container = translated_soup.body if translated_soup.body is not None else translated_soup

    original_paragraphs = original_container.find_all('p')
    translated_paragraphs = translated_container.find_all('p')

    if len(translated_paragraphs) != len(original_paragraphs):
        logger.warning("ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ê°„ <p> íƒœê·¸ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¸ë‹¨ ë³µì›ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return translated_container.decode_contents()

    for idx, trans_p in enumerate(translated_paragraphs):
        if pattern.search(trans_p.get_text()):
            trans_p.replace_with(copy.deepcopy(original_paragraphs[idx]))

    result_html = translated_container.decode_contents()

    # í›„ì²˜ë¦¬: case-sensitive ì†ì„± ë³µì›ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    result_html = restore_case_sensitive_attribs(result_html)

    return result_html

def parse_retry_delay_from_error(error, extra_seconds=2) -> int:
    """
    ì£¼ì–´ì§„ ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì¬ì‹œë„ ì§€ì—°ì‹œê°„(ì´ˆ)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì—ëŸ¬ ë©”ì‹œì§€ì— "retry after <ìˆ«ì>" í˜•íƒœê°€ ìˆë‹¤ë©´ í•´ë‹¹ ìˆ«ìë¥¼ ë°˜í™˜í•˜ê³ ,
    ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ 10ì´ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    error_str = str(error)
    start_idx = error_str.find('{')
    if start_idx == -1:
        return 0  # JSON ë¶€ë¶„ ì—†ìŒ
    
    dict_str = error_str[start_idx:]
    retry_seconds = 0

    try:
        # JSONì´ ì•„ë‹ˆë¼ Python dictì²˜ëŸ¼ ìƒê¸´ ë¬¸ìì—´ì´ë¯€ë¡œ literal_eval ì‚¬ìš©
        err_data = ast.literal_eval(dict_str)
        details = err_data.get("error", {}).get("details", [])
        for detail in details:
            if detail.get("@type") == "type.googleapis.com/google.rpc.RetryInfo":
                retry_str = detail.get("retryDelay", "")  # ì˜ˆ: "39s"
                match = re.match(r"(\d+)s", retry_str)
                if match:
                    retry_seconds = int(match.group(1))
                    break
    except Exception as e:
        print(f"[WARN] retryDelay íŒŒì‹± ì‹¤íŒ¨: {e}")
        return 10

    return retry_seconds + extra_seconds if retry_seconds > 0 else 0

# Setup logger
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler_stream = logging.StreamHandler()
    handler_file = logging.FileHandler('translator.log', encoding='utf-8')
    formatter = logging.Formatter('[%(asctime)s] %(levelname)s:%(name)s: %(message)s')
    handler_stream.setFormatter(formatter)
    handler_file.setFormatter(formatter)
    logger.addHandler(handler_stream)
    logger.addHandler(handler_file)
    logger.setLevel(logging.INFO)

client = None
max_chunk_size = 3000
llm_model = 'gemini-2.0-flash'
custom_prompt = ''
llm_delay = 0.0
japanese_char_threshold = 15
language = 'Japanese'

def set_client(client_instance):
    """Sets the global client instance for API calls."""
    global client
    client = client_instance

def set_llm_model(name):
    global llm_model
    llm_model = name

def set_chunk_size(size):
    global max_chunk_size
    max_chunk_size = size

def set_custom_prompt(prompt):
    global custom_prompt
    if prompt:
        custom_prompt = prompt

def set_llm_delay(time):
    global llm_delay
    llm_delay = time

def set_japanese_char_threshold(threshold):
    global japanese_char_threshold
    japanese_char_threshold = threshold
    
def set_language(lang):
    global language
    language = lang

def clean_gemini_response(response_text: str) -> str:
    """
    Cleans the Gemini API response by stripping markdown formatting.
    """
    text = response_text.strip()
    if text.startswith("```html"):
        text = text[7:].strip()
    elif text.startswith("```"):
        text = text[3:].strip()
    if text.startswith("### html"):
        text = text[8:].strip()
    if text.startswith("```html"):
        text = text[7:].strip()
    elif text.startswith("```"):
        text = text[3:].strip()
    if text.endswith("```"):
        text = text[:-3].strip()
    return text

def detect_repeats(text: str, min_repeat: int = 4, max_unit_len: int = 10) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ì„ <repeat time="N">...<repeat> í˜•íƒœë¡œ ê°ì‹¸ì„œ ë°˜í™˜
    """
    i = 0
    result = ""
    text_len = len(text)

    while i < text_len:
        replaced = False
        for unit_len in range(max_unit_len, 0, -1):
            unit = text[i:i + unit_len]
            if not unit or i + unit_len > text_len:
                continue

            repeat_count = 1
            while text[i + repeat_count * unit_len: i + (repeat_count + 1) * unit_len] == unit:
                repeat_count += 1

            if repeat_count >= min_repeat:
                result += f'<repeat time="{repeat_count}">{unit}</repeat>'
                i += unit_len * repeat_count
                replaced = True
                break

        if not replaced:
            result += text[i]
            i += 1

    return result

def wrap_repeats_in_paragraphs_and_headers(html: str, min_repeat: int = 4) -> str:
    """
    HTMLì—ì„œ <p> ë° <h1>~<h6> íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ì—ë§Œ detect_repeats() ì ìš©
    BeautifulSoup ì—†ì´ ì²˜ë¦¬í•˜ì—¬ ì›ë³¸ íƒœê·¸ êµ¬ì¡° ë³´ì¡´
    """
    def process_tag(match):
        tag = match.group(1)
        attrs = match.group(2) or ""  # Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©
        inner = match.group(3)
        processed_inner = detect_repeats(inner, min_repeat=min_repeat)
        return f"<{tag}{attrs}>{processed_inner}</{tag}>"

    # <p> ë° <h1>~<h6> íƒœê·¸ë§Œ ì²˜ë¦¬
    pattern = re.compile(r'<(p|h[1-6])(\s[^>]*)?>(.*?)</\1>', re.DOTALL)
    return pattern.sub(process_tag, html)


def restore_repeat_tags_translated(html_content: str) -> str:
    """
    HTML ë‚´ ì´ìŠ¤ì¼€ì´í”„ëœ <repeat time="N">...</repeat> íƒœê·¸ë¥¼ ì‹¤ì œ ë°˜ë³µ ë¬¸ìì—´ë¡œ ë³µì›
    """
    # 1. HTML ë¬¸ìì—´ ë‚´ ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ì(&lt;, &gt; ë“±)ë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜
    unescaped_html = html.unescape(html_content)
    
    # 2. ì •ê·œì‹ íŒ¨í„´: <repeat time="N">...</repeat> (ë‹¨ìˆœ ì¤‘ì²© ì—†ì´ ì²˜ë¦¬)
    pattern = re.compile(r'<repeat\s+time="(\d+)">(.*?)</repeat>', re.DOTALL)
    
    # ë°˜ë³µì ìœ¼ë¡œ ì°¾ì•„ì„œ êµì²´ (íƒœê·¸ ì•ˆì— ë˜ repeatê°€ ìˆì„ ê²½ìš°ì—ë„ ì²˜ë¦¬)
    while True:
        match = pattern.search(unescaped_html)
        if not match:
            break
        
        count = int(match.group(1))
        content = match.group(2)
        repeated_content = content * count
        
        unescaped_html = unescaped_html[:match.start()] + repeated_content + unescaped_html[match.end():]
    
    return unescaped_html

def count_closing_tags(html: str, tags: list[str]) -> dict:
    """
    HTML ë¬¸ìì—´ì—ì„œ ì§€ì •ëœ íƒœê·¸ë“¤ì˜ ë‹«ëŠ” íƒœê·¸(</tag>) ê°œìˆ˜ë¥¼ ë°˜í™˜
    """
    counts = {}
    for tag in tags:
        closing_pattern = f"</{tag}>"
        counts[tag] = html.count(closing_pattern)
    return counts
def translate_text_simple(text, language):
    prompt = (
        "ë‹¹ì‹ ì€ ë¼ì´íŠ¸ë…¸ë²¨ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.\n\n"

        "ğŸ“ ë²ˆì—­ ì§€ì¹¨:\n"
        "- **ì´ë¯¸ í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ë¬¸ì¥ì€ ìˆ˜ì •í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**\n"
        "- **í•œêµ­ì–´ê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ë§Œ ìì—°ìŠ¤ëŸ½ê³  ë¬¸í•™ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤.**\n"
        "- **ì¶œë ¥ì—ëŠ” ì˜¤ì§ í•œêµ­ì–´ë§Œ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, ì™¸êµ­ì–´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**\n"
        "- **ì™¸êµ­ì–´ ì›ë¬¸ì„ ë³‘ê¸°í•˜ê±°ë‚˜ '(ì›ë¬¸)â†’(ë²ˆì—­ë¬¸)'ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì ˆëŒ€ í‘œì‹œí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**\n"
        "- ì„¤ëª…, ì£¼ì„, ë§ˆí¬ë‹¤ìš´ ë“±ì˜ ì¶œë ¥ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
        "- 'CONTENTS'ëŠ” 'ëª©ì°¨'ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤."

        "ë‹¤ìŒ ê¸€ì„ ê²€í† í•˜ì—¬ ì™¸êµ­ì–´ë§Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤. ì´ë¯¸ ë²ˆì—­ëœ í•œêµ­ì–´ëŠ” ë³€ê²½í•˜ì§€ ë§ˆì‹­ì‹œì˜¤:\n\n" + text
    )
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.models.generate_content(
                model=llm_model,
                contents=prompt,
                config=types.GenerateContentConfig(
                max_output_tokens=8192 if max_chunk_size < 8192 else max_chunk_size,
            ),
            )
            output = response.text.strip()
            output = clean_gemini_response(output)
            output = restore_repeat_tags_translated(output)

            logger.info(
                f"Translation result:\n"
                f"--- Input HTML ---\n{text}\n"
                f"--- Output HTML ---\n{output}"
            )

            if not output:
                raise ValueError("Empty or non-HTML response from Gemini for translation.")
            time.sleep(llm_delay)
            return output
        except Exception as e:
            logger.error(f"translate_text_simple - Error on attempt {attempt}: {e}")
            if "429" in str(e) or "Resource exhausted" in str(e):
                attempt -= 1
                delay = parse_retry_delay_from_error(e)
                logger.info(f"429 error detected in translate_text_simple, retrying after {delay} seconds.")
                time.sleep(delay)
            else:
                time.sleep(5 * attempt)
    logger.error("translate_text_simple - Final failure after MAX_RETRIES attempts.")
    raise Exception("Translation failed in translate_text_simple.")

def remove_russian_from_text(text):
    prompt = (
        'ë‹¹ì‹ ì€ ë¬¸ì¥ì— ì„ì—¬ìˆëŠ” ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ì ëŠ” ë²ˆì—­ê°€ì…ë‹ˆë‹¤.'
        'ë¬¸ì¥ì†ì— ìˆëŠ” ë‹¤ë¥¸ ìš”ì†ŒëŠ” ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ê³ , ëŸ¬ì‹œì•„ì–´ë¡œ ì í˜€ìˆëŠ” ë‹¨ì–´ë§Œ í•œêµ­ì–´ë¡œ ë°”ê¿” ë¬¸ì¥ì„ ì™„ì„± í•˜ì‹­ì‹œì˜¤.'
        'ë‹¤ìŒ ë¬¸ì¥ì˜ ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´ë¥¼ ë°”ê¿” ì™„ì„±ëœ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤, ë²ˆì—­ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.\n\n' + text
    )
    try:
        response = client.models.generate_content(
            model=llm_model,
            contents=prompt,    
            config=types.GenerateContentConfig(
            max_output_tokens=8192 if max_chunk_size < 8192 else max_chunk_size,
            top_k=40,
            top_p=0.85,
            temperature=1.8,
            )
        )
        return response.text.strip()
    except Exception as e:
        return text

def translate_chunk_for_enhance(html_fragment, language):
    set_language(language)
    processed_html = detect_repeats(html_fragment)
    prompt = (
        'ë‹¹ì‹ ì€ ë¼ì´íŠ¸ë…¸ë²¨ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì— ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ ì´ˆë²Œ ë²ˆì—­ë³¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë¶€ ì™¸êµ­ì–´ê°€ ê·¸ëŒ€ë¡œ ë‚¨ì•„ ìˆê±°ë‚˜, ë²ˆì—­ì´ ì–´ìƒ‰í•œ ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤.'
        'ğŸ“ ë²ˆì—­ ì§€ì¹¨:'
        '- ëª¨ë“  ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê³  ë¬¸í•™ì ì¸ í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ë‹¤ë“¬ì–´ ì£¼ì‹­ì‹œì˜¤.'
        '- ê³ ìœ ëª…ì‚¬(ì¸ëª…, ì§€ëª…, ì‘í’ˆëª… ë“±)ëŠ” ì›ì–´ ë°œìŒì„ ê¸°ì¤€ìœ¼ë¡œ ìŒì—­í•´ ì£¼ì‹­ì‹œì˜¤.'
        '- ì›ë¬¸ì˜ ì˜ë¯¸ë‚˜ ë…¼ë¦¬ë¥¼ ì„ì˜ë¡œ í•´ì„í•˜ê±°ë‚˜ ì°½ì‘í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.'
        "- ì¶œë ¥ì—ëŠ” ì˜¤ì§ í•œêµ­ì–´ë§Œ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, ì–´ë– í•œ ì™¸êµ­ì–´ ë‹¨ì–´ë‚˜ ë¬¸ì¥ë„ í¬í•¨ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤."
        "- ã€Œ ã€, ã€ ã€ ë¬¸ì¥ë¶€í˜¸ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , ë‹«íˆì§€ ì•Šì•˜ë”ë¼ë„ ì„ì˜ë¡œ ë‹«ì§€ ë§ˆì‹­ì‹œì˜¤."
        "- ì›ë¬¸ ë³‘ê¸°, ì£¼ì„, ì„¤ëª… ë“±ì€ ì ˆëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n"
        "ğŸ“Œ ë‹¤ìŒ ë¬¸ì¥ì„ ì² ì €íˆ ì§€ì¹¨ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê³  ë¬¸í•™ì ìœ¼ë¡œ ë‹¤ì‹œ ë²ˆì—­í•´ ì£¼ì‹­ì‹œì˜¤ ë²ˆì—­ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì‹œì˜¤:\n\n"+ processed_html
    )
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.models.generate_content(
                model=llm_model,
                contents=prompt,    
                config=types.GenerateContentConfig(
                max_output_tokens=8192 if max_chunk_size < 8192 else max_chunk_size,
                top_k=40,
                top_p=0.85,
                temperature=1.8,
            ),
            )
            output = response.text.strip()
            output = clean_gemini_response(output)
            output = restore_repeat_tags_translated(output)

            logger.info(
                f"Translation result:\n"
                f"--- Input HTML ---\n{html_fragment}\n"
                f"--- Output HTML ---\n{output}"
            )

            if not output:
                raise ValueError("Empty or non-HTML response from Gemini for translation.")
            if len(output) > 3 * len(processed_html):
                raise ValueError("Repetition detected")
            cyrill_chars = re.findall(r'[\u0400-\u04FF\u0500-\u052F]', output)
            if len(cyrill_chars) > 0:
                output = remove_russian_from_text(output)
            
            time.sleep(llm_delay)
            return output
        except Exception as e:
            logger.error(f"translate_chunk_for_enhance - Error on attempt {attempt}: {e}")
            if "429" in str(e) or "Resource exhausted" in str(e):
                attempt -= 1
                delay = parse_retry_delay_from_error(e)
                logger.info(f"429 error detected in translate_chunk_for_enhance, retrying after {delay} seconds.")
                time.sleep(delay)
            else:
                time.sleep(5 * attempt)
    logger.error("translate_chunk_for_enhance - Final failure after MAX_RETRIES attempts.")
    raise Exception("Translation failed in translate_chunk_for_enhance.")

def translate_chunk_with_html(html_fragment, chapter_index, chunk_index, language):
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ë³¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
    ë²ˆì—­ì— ì‹¤íŒ¨í•  ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
    """
    set_language(language)
    preprocessed_html = wrap_repeats_in_paragraphs_and_headers(html_fragment)
    prompt = (
        "**ì¤‘ìš”:** ë°˜ë“œì‹œ **ìˆœìˆ˜í•˜ê²Œ ë²ˆì—­ëœ HTMLë§Œ** ë°˜í™˜í•˜ì‹­ì‹œì˜¤. ì„¤ëª…, ì£¼ì„, ì½”ë“œ ë¸”ë¡ ë“± ë¶€ê°€ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n"
        f"ë‹¹ì‹ ì€ {language} ë¼ì´íŠ¸ë…¸ë²¨ ì „ë¬¸ ë²ˆì—­ê°€ì´ë©°, {language}ì—ì„œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. ë²ˆì—­ì€ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš°ë©° ê°ì • í‘œí˜„ì´ í’ë¶€í•´ì•¼ í•˜ê³ , ì¶œê°„ì— ì í•©í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        "ğŸ¯ ë²ˆì—­ ì§€ì¹¨:\n"
        "- ì›ë¬¸ì˜ ì–´ì¡°, ë¬¸í•™ì  ë‰˜ì•™ìŠ¤, ëŒ€í™”ì²´ ìŠ¤íƒ€ì¼ ìœ ì§€\n"
        "- ëª°ì…ê° ìˆê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ì‚¬ìš©\n\n"
        "- ë¹„ì¼ìƒì ì¸ í•œìì–´ëŠ” ë¬¸ë§¥ì— ë§ëŠ” í˜„ëŒ€ í•œêµ­ì–´ í‘œí˜„ìœ¼ë¡œ ì˜ì—­\n"
        "- ì´ë¯¸ ë²ˆì—­ëœ ê³ ìœ ëª…ì‚¬ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n"
        "- æ€ã‚ãšëŠ” ë¬´ì‹¬ì½”ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤"
        "âš ï¸ HTML ë° í˜•ì‹ ê´€ë ¨:\n"
        "- HTML íƒœê·¸ ë° êµ¬ì¡°(`&lt;p&gt;`, `&lt;img&gt;`, `&lt;repeat&gt;`, `class` ë“±)ëŠ” ë³€ê²½, ì œê±°, ì¬ë°°ì—´í•˜ì§€ ì•ŠìŒ\n"
        "- íŒŒì¼ ê²½ë¡œ, ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸, href, í´ë˜ìŠ¤ ì´ë¦„, ë©”íƒ€ë°ì´í„° ë“± ë³´ì´ì§€ ì•ŠëŠ” ì •ë³´ëŠ” ë²ˆì—­í•˜ì§€ ì•ŠìŒ\n"
        f"- ìµœì¢… ê²°ê³¼ì— {language} ë¬¸ìê°€ ë‚¨ìœ¼ë©´ ì•ˆ ë¨\n"
        "- ë²ˆì—­í•  ì™¸êµ­ì–´ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì›ë¬¸ HTML ê·¸ëŒ€ë¡œ ë°˜í™˜\n\n"
        + custom_prompt +
        "\nì´ì œ ë‹¤ìŒ HTMLì„ ê²€í† í•˜ì—¬ ë²ˆì—­í•˜ì„¸ìš”:\n\n" + preprocessed_html
    )

    response = client.models.generate_content(
        model=llm_model,
        contents=prompt,
        config=types.GenerateContentConfig(
            top_p=0.8,
            temperature=1.8,
            max_output_tokens=8192 if max_chunk_size < 8192 else max_chunk_size,
        ),
    )
    # response.textê°€ Noneì¸ ê²½ìš°ë¥¼ í™•ì¸í•˜ê³  ë¡œê¹…
    if response.text is None:
        logger.error(
            f"[CH{chapter_index}][CHUNK{chunk_index}] API ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ Noneì…ë‹ˆë‹¤. "
            f"ì „ì²´ ì‘ë‹µ ê°ì²´: {response}"
        )
        raise ValueError("API response text is None, triggering retry.") # ì¬ì‹œë„ë¥¼ ìœ ë°œí•˜ê¸° ìœ„í•´ ì˜ˆì™¸ ë°œìƒ
    output = response.text.strip() 
    output = clean_gemini_response(output)
    output = restore_repeat_tags_translated(output)

    logger.info(
        f"[CH{chapter_index}][CHUNK{chunk_index}] ë²ˆì—­ ê²°ê³¼:\n"
        f"--- ì›ë¬¸ HTML ---\n{html_fragment}\n"
        f"--- ì „ì²˜ë¦¬ HTML ---\n{preprocessed_html}\n"
        f"--- ë²ˆì—­ ê²°ê³¼ HTML ---\n{output}"
    )

    input_tag_counts = count_closing_tags(html_fragment, ["p"] + [f"h{i}" for i in range(1, 7)])
    output_tag_counts = count_closing_tags(output, ["p"] + [f"h{i}" for i in range(1, 7)])

    if input_tag_counts["p"] != output_tag_counts["p"]:
        raise ValueError(f"<p> íƒœê·¸ ê°œìˆ˜ ë¶ˆì¼ì¹˜: input={input_tag_counts['p']} / output={output_tag_counts['p']}")

    input_h_total = sum(input_tag_counts[f"h{i}"] for i in range(1, 7))
    output_h_total = sum(output_tag_counts[f"h{i}"] for i in range(1, 7))
    if input_h_total != output_h_total:
        raise ValueError(f"<h1>~<h6> íƒœê·¸ ì´ ê°œìˆ˜ ë¶ˆì¼ì¹˜: input={input_h_total} / output={output_h_total}")

    if not output or "<" not in output:
        error_message = f"chapter {chapter_index}, chunk {chunk_index}ì— ëŒ€í•´ Geminië¡œë¶€í„° ë¹ˆë²ˆì—­ ë˜ëŠ” HTML í˜•ì‹ì´ ì•„ë‹˜."
        logger.error(error_message)
        raise ValueError(error_message)

    time.sleep(llm_delay)
    return output

def annotate_image(img_bytes, language):
    set_language(language)
    prompt = (
        "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ì†ì— í¬í•¨ëœ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ê²Œ ë©ë‹ˆë‹¤.\n"
        "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n"

        "ğŸ“ ë²ˆì—­ ì§€ì¹¨:\n"
        "- ì´ë¯¸ì§€ì— **ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë§Œ** ë²ˆì—­í•˜ì‹­ì‹œì˜¤.\n"
        "- í•œêµ­ì–´ ì›ì–´ë¯¼ì´ ì½ê¸°ì— ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n"
        "- ì¶œë ¥ì—ëŠ” **í•œêµ­ì–´ë§Œ í¬í•¨**ë˜ì–´ì•¼ í•˜ë©°, ì™¸êµ­ì–´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
        "- ì„¤ëª…, ì£¼ì„, ë§ˆí¬ë‹¤ìš´ ë“±ì˜ í˜•ì‹ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n"
        "- ë³´ì´ëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ í…ìŠ¤íŠ¸('')ë¥¼ ë°˜í™˜í•˜ì‹­ì‹œì˜¤."

        "ì˜¤ì§ ë²ˆì—­ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."
    )
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.models.generate_content(
                model=llm_model,
                contents=[
                    prompt,
                    PIL.Image.open(io.BytesIO(img_bytes))
                ],
                config=types.GenerateContentConfig(
                top_k= 40,
                top_p= 0.85,
                temperature= 0.8,
                max_output_tokens=8192 if max_chunk_size < 8192 else max_chunk_size,
                frequency_penalty=0.5,
                ),
            )
            if response.text:
                output_text = response.text.strip()
            elif response.text == "''" or response.text == None:
                output_text = ""
            logger.info(f"--- Output Annotation ---\n{output_text}")
            return output_text
        except Exception as e:
            logger.error(f"annotate_image - Error on attempt {attempt}: {e}")
            if "429" in str(e) or "Resource exhausted" in str(e):
                attempt -= 1
                delay = parse_retry_delay_from_error(e)
                logger.info(f"429 error detected in annotate_image, retrying after {delay} seconds.")
                time.sleep(delay)
            else:
                time.sleep(5 * attempt)
    logger.error("annotate_image - Final failure after MAX_RETRIES attempts.")
    raise Exception("Annotate image failed.")

async def async_translate_chunk(html_fragment, chapter_index, chunk_index, semaphore, executor):
    """
    HTML ì²­í¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë²ˆì—­í•˜ë©´ì„œ ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
    ë§ˆì§€ë§‰ ìµœì¢… ê²°ê³¼ì— ëŒ€í•´ì„œë§Œ ê° <p> íƒœê·¸ ë‚´ ì™¸êµ­/ì¼ë³¸ì–´ ë¬¸ìê°€ ë°œê²¬ë˜ë©´ ì›ë¬¸ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.
    """
    final_result = None
    for attempt in range(1, MAX_RETRIES + 1):
        async with semaphore:
            try:
                loop = asyncio.get_running_loop()
                result = await loop.run_in_executor(
                    executor,
                    translate_chunk_with_html,
                    html_fragment,
                    chapter_index,
                    chunk_index,
                    language
                )
                # ë²ˆì—­ ê²°ê³¼ì—ì„œ HTML íƒœê·¸ ì œì™¸ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ
                soup = BeautifulSoup(result, "lxml-xml")
                visible_text = soup.get_text()
                japanese_chars = re.findall(r'[\u3040-\u30FF\u31F0-\u31FF\u4E00-\u9FFF]', visible_text)
                cyrill_chars = re.findall(r'[\u0400-\u04FF\u0500-\u052F]', visible_text)
                thai_chars = re.findall(r'[\u0E00-\u0E7F]', visible_text)
                arabic_chars = re.findall(r'[\u0600-\u06FF\u0750-\u077F]', visible_text)
                hebrew_chars = re.findall(r'[\u0590-\u05FF]', visible_text)
                devanagari_chars = re.findall(r'[\u0900-\u097F]', visible_text)
                greek_chars = re.findall(r'[\u0370-\u03FF]', visible_text)
                
                if japanese_chars or cyrill_chars or thai_chars or arabic_chars or hebrew_chars or devanagari_chars or greek_chars:
                    foreign_chars = japanese_chars + cyrill_chars + thai_chars + arabic_chars + hebrew_chars + devanagari_chars + greek_chars
                else:
                    foreign_chars = []
                count = len(foreign_chars)
                if count >= japanese_char_threshold:
                    if attempt < MAX_RETRIES:
                        raise Exception(
                            f"ë²ˆì—­ ê²°ê³¼ì— {count}ê°œì˜ ì¼ë³¸ì–´(ë˜ëŠ” ì™¸êµ­) ë¬¸ìê°€ í¬í•¨ë˜ì–´ attempt {attempt}ì—ì„œ ì¬ì‹œë„ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤."
                        )
                    else:
                        logger.warning(
                            f"[{chapter_index}-{chunk_index}] ìµœì¢… ì‹œë„ ê²°ê³¼ì— {count}ê°œì˜ ì¼ë³¸ì–´/ì™¸êµ­ ë¬¸ì í¬í•¨. í•´ë‹¹ ê²°ê³¼ ì‚¬ìš©."
                        )
                final_result = result
                break  # ì„±ê³µì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°›ì•„ì™”ìœ¼ë¯€ë¡œ ë£¨í”„ ì¢…ë£Œ
            except Exception as e:
                logger.error(f"[{chapter_index}-{chunk_index}] attempt {attempt} ì—ì„œ ì—ëŸ¬ ë°œìƒ: {e}")
                if "429" in str(e) or "Resource exhausted" in str(e):
                    # 429 ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„ ì „ ë”œë ˆì´
                    attempt -= 1
                    delay = parse_retry_delay_from_error(e)
                    logger.info(f"[{chapter_index}-{chunk_index}] 429 ì—ëŸ¬ ê°ì§€, {delay}ì´ˆ í›„ ì¬ì‹œë„.")
                    await asyncio.sleep(delay)
                else:
                    await asyncio.sleep(5 * attempt)
            finally:
                await asyncio.sleep(llm_delay)
    
    if final_result is None:
        logger.error(f"[{chapter_index}-{chunk_index}] ìµœëŒ€ ì¬ì‹œë„({MAX_RETRIES}íšŒ) í›„ ìµœì¢… ì‹¤íŒ¨. ì›ë¬¸ ì²­í¬ ë°˜í™˜.")
        final_result = html_fragment
    
    # ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ ìµœì¢… ê²°ê³¼ì— ëŒ€í•´ ê° <p> íƒœê·¸ë¥¼ ê²€ì‚¬í•˜ì—¬
    # ì™¸êµ­/ì¼ë³¸ì–´ ë¬¸ìê°€ ìˆìœ¼ë©´ ì›ë¬¸ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.
    final_result = revert_foreign_paragraphs(html_fragment, final_result)
    return final_result

async def translate_chapter_async(html, chapter_index, executor, semaphore):
    """
    Translates an entire chapter asynchronously by splitting it into chunks,
    translating each chunk concurrently, and recombining the results.
    """
    soup = BeautifulSoup(html, 'lxml-xml')
    head = soup.head
    body = soup.body if soup.body else soup

    # Use HtmlChunker to split the body into manageable chunks.
    chunker = HtmlChunker(max_chars=max_chunk_size)
    chunks = chunker.chunk_body_preserving_structure(body)

    tasks = []
    for chunk_index, chunk in enumerate(chunks):
        html_fragment = str(chunk)
        tasks.append(
            async_translate_chunk(html_fragment, chapter_index, chunk_index, semaphore, executor)
        )
    translated_chunks = await asyncio.gather(*tasks)

    new_soup = BeautifulSoup("", "lxml-xml")
    html_tag = new_soup.new_tag("html")
    new_soup.append(html_tag)
    if head:
        html_tag.append(copy.copy(head))
    new_body = new_soup.new_tag("body")
    for translated_chunk in translated_chunks:
        chunk_soup = BeautifulSoup(translated_chunk, "lxml-xml")
        for content in list(chunk_soup.contents):
            new_body.append(content)
    html_tag.append(new_body)
    return str(new_soup)
